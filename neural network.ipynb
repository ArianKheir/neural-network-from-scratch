{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a Neural network from scratch</h1>\n",
    "<p>We are going to use just numpy for calculations with matrices and matplotlib.pyplot for showing results on charts</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU:</h4>\n",
    "<p>We use ReLU to set all the negative inputs to zero and leave positive inputs unchanged.<br>\n",
    "It's simplicity makes it computationally efficient, especially in large neural networks.<br>\n",
    "ReLU introduces non-linearity into the network, allowing it to learn complex patterns.<br>\n",
    "For z > 0 the gradient is constant (dReLu(z)/dz =1), which help us avoid the vanishing gradient problem (which is common in sigmoid/tanh). <br>\n",
    "Also it sets some activations to zero, which can help with computational efficiency and reduce the risk of overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Softmax Function:</h4>\n",
    "<p>It converts raw scores z(logits) into probabilities and also ensures that the probabilities sum to 1 across all classes.<br>\n",
    "Exponentiating the logits ensures all values are positive and also Larger logits get amplified more than smaller logits.<br>\n",
    "Dividing by the sum of all exponentiated logits ensures the output values are probabilities is also some kind of normalization.The subtraction of max(Z) prevents numerical overflow(e.g for large exponentials).</p>\n",
    "<p>In code below axis=0 ensures softmax is computed for each column(e.g for each class in a multi-class classification setting).<br>\n",
    "And Also keepdims=True ensures that the result sum has the same dimensions as expZ, allowing for element-wise division without broadcasting issues.</p>\n",
    "<h5>fromula for it is:</h5>\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross entropy loss function</h4>\n",
    "<p>This function computes the cross-entropy loss, which measures the difference between the predicted probabilities (y_pred) and the true labels (y_true). It is commonly used in classification tasks, especially for multi-class classification with softmax outputs.</p>\n",
    "<h5>fromula for this loss function is:</h5>\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$\n",
    "\n",
    "If yk = 1(the correct class), the loss for that sample is equal to:\n",
    "\n",
    "$$L = -\\log(\\hat{y}_k)$$\n",
    "Encourages hat{yk} (the predicted probability for the correct class) to approach 1.<br>\n",
    "The small value 1e-8 is added to hat{yk} to prevent taking the logarithm of 0, which would result in an undefined value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>When Combined with Softmax Output:</h5>\n",
    "<p>After simplifying, the gradient of the combined softmax and cross-entropy loss is:<br></p>\n",
    "\n",
    "$$\\frac{\\partial z_k^{(i)}}{\\partial J} = \\hat{y}_k^{(i)} - y_k^{(i)}$$\n",
    "<p>And when we Stack the derivatives for all m examples into a matrix: </p>\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial J} = \\hat{Y} - Y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[1]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
