{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a Neural network from scratch</h1>\n",
    "<p>We are going to use just numpy for calculations with matrices and matplotlib.pyplot for showing results on charts</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU:</h4>\n",
    "<p>We use ReLU to set all the negative inputs to zero and leave positive inputs unchanged.<br>\n",
    "It's simplicity makes it computationally efficient, especially in large neural networks.<br>\n",
    "ReLU introduces non-linearity into the network, allowing it to learn complex patterns.<br>\n",
    "For z > 0 the gradient is constant (dReLu(z)/dz =1), which help us avoid the vanishing gradient problem (which is common in sigmoid/tanh). <br>\n",
    "Also it sets some activations to zero, which can help with computational efficiency and reduce the risk of overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Softmax Function:</h4>\n",
    "<p>It converts raw scores z(logits) into probabilities and also ensures that the probabilities sum to 1 across all classes.<br>\n",
    "Exponentiating the logits ensures all values are positive and also Larger logits get amplified more than smaller logits.<br>\n",
    "Dividing by the sum of all exponentiated logits ensures the output values are probabilities is also some kind of normalization.The subtraction of max(Z) prevents numerical overflow(e.g for large exponentials).</p>\n",
    "<p>In code below axis=0 ensures softmax is computed for each column(e.g for each class in a multi-class classification setting).<br>\n",
    "And Also keepdims=True ensures that the result sum has the same dimensions as expZ, allowing for element-wise division without broadcasting issues.</p>\n",
    "<h5>fromula for it is:</h5>\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross entropy loss function</h4>\n",
    "<p>This function computes the cross-entropy loss, which measures the difference between the predicted probabilities (y_pred) and the true labels (y_true). It is commonly used in classification tasks, especially for multi-class classification with softmax outputs.</p>\n",
    "<h5>fromula for this loss function is:</h5>\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$\n",
    "\n",
    "If yk = 1(the correct class), the loss for that sample is equal to:\n",
    "\n",
    "$$L = -\\log(\\hat{y}_k)$$\n",
    "Encourages hat{yk} (the predicted probability for the correct class) to approach 1.<br>\n",
    "The small value 1e-8 is added to hat{yk} to prevent taking the logarithm of 0, which would result in an undefined value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>When Combined with Softmax Output:</h5>\n",
    "<p>After simplifying, the gradient of the combined softmax and cross-entropy loss is:<br></p>\n",
    "\n",
    "$$\\frac{\\partial z_k^{(i)}}{\\partial J} = \\hat{y}_k^{(i)} - y_k^{(i)}$$\n",
    "<p>And when we Stack the derivatives for all m examples into a matrix: </p>\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial J} = \\hat{Y} - Y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[1]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Initializing the weights and biases</h4>\n",
    "<p>Proper initialization of weights is critical for training the network effectively, as it impacts the convergence speed and overall performance.<br>\n",
    "Poor initialization (e.g., very large or very small weights) can cause gradients to shrink (vanish) or grow (explode) exponentially as they propagate through layers.<br>\n",
    "We set a seed for NumPy's random number generator to ensure reproducibility.<br>\n",
    "Using the same seed produces the same random numbers every time the code runs, which is useful for debugging or comparing results.<br>\n",
    "For initializing weights we use He initialization.<br>\n",
    "For each weight we generate random values from a standard normal distribution(N(0, 1)) and scale it by * np.sqrt(2 / layer_dims[l-1])<br>\n",
    "Using He initialization ensures the variance of the activations is maintained as the signal passes through layers, avoiding the problems of vanishing or exploding gradients<br></p>\n",
    "<p>We initialize the biases to zeros by np.zeros()<br>\n",
    "This doesn‚Äôt break symmetry, unlike initializing weights to zeros (which would cause all neurons in the layer to learn identical features).</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dropout</h4>\n",
    "<p>Dropout is a regularization technique where a random subset of neurons is \"dropped\" (set to 0) during training. This prevents the network from becoming too reliant on specific neurons and helps generalization.<br>\n",
    "the process works like below:<br>\n",
    "if dropout is enabled (keep_prob < 1.0):<br>\n",
    "We create a random matrix D where each element is True with probability keep_prob and False otherwise.<br>\n",
    "Then we drop some neurons in A by element-wise multiplying A and D and after that we scale the remaining activations by dividing by keep_prob.This will help us to maintain the expected value of the activations.After all, we store the dropout mask D in the cache.<br>\n",
    "<h5>There are many benefits using this regularization technique:</h5>\n",
    "<p>By randomly deactivating neurons, dropout prevents the network from relying too much on specific neurons.<br>\n",
    "Also this technique forces the network to learn redundant representations since no single neuron can dominate.<br></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward propagation</h4>\n",
    "<h5>In this function we calculates the outputs of each layer in the neural network from the input layer to the output layer.</h5>\n",
    "<p>L is the total number of layers in the network (excluding the input layer). Each layer has weights and biases, hence parameters contains 2 * L keys.<br>\n",
    "A is initially set to the input matrix X and cache is used to store intermediate values (activations A and pre-activations Z) for each layer, which will be needed during backward propagation.<br>\n",
    "For each layer l we compute the pre-activation Z using the formula:<br>\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "where W[l] is the weight matrix for layer l.<br> A[l‚àí1] is the activation from the previous layer (or input X for the first layer) <br>and b[l] is the bias vector for layer l.<br>\n",
    "Then we compute the activation A using the ReLU activation function:\n",
    "$$A^{[l]} = \\text{ReLU}(Z^{[l]})$$\n",
    "We also use Dropout Regularization the way we described above<br></p>\n",
    "<p>in final layer:</p>\n",
    "<p>we compute the pre-activation ZL for the final layer using the same formula as above.\n",
    "then we compute the activation AL using the softmax function<br>\n",
    "After all, We store the pre-activation ZL and activation AL for the final layer in the cache and Al will be our final output of the network</p>\n",
    "<p>To put it in a nutshell:<br>\n",
    "    We applie the weight and bias transformations(Z) layer-by-layer, starting from the input layer and moving toward the output layer.<br>\n",
    "    Then we use non-linear activation functions (ReLU, softmax) to introduce non-linearity, allowing the network to model complex relationships.<br>\n",
    "    Also we optionally use dropout to prevent overfitting by randomly deactivating neurons during training.<br>\n",
    "    And in the end we store all intermediate values in cache, ensuring the data needed for backpropagation is available<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob=1.0):\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "    cache = {\"A0\": A}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters[f\"W{l}\"], A) + parameters[f\"b{l}\"]\n",
    "        A = relu(Z)\n",
    "        \n",
    "        if keep_prob < 1.0:\n",
    "            D = np.random.rand(A.shape[0], A.shape[1]) < keep_prob\n",
    "            A = A * D\n",
    "            A = A / keep_prob\n",
    "            cache[f\"D{l}\"] = D\n",
    "        \n",
    "        cache[f\"Z{l}\"] = Z\n",
    "        cache[f\"A{l}\"] = A\n",
    "\n",
    "    ZL = np.dot(parameters[f\"W{L}\"], A) + parameters[f\"b{L}\"]\n",
    "    AL = softmax(ZL)\n",
    "    \n",
    "    cache[f\"Z{L}\"] = ZL\n",
    "    cache[f\"A{L}\"] = AL\n",
    "    return AL, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>L2 regularization</h4>\n",
    "<h5>L2 regularization, also known as weight decay, is a technique used in machine learning to prevent overfitting by discouraging large weights in the model. It works by adding a penalty term to the cost function, which is proportional to the sum of the squared weights of the model. This encourages the model to keep the weights small, leading to a simpler and more generalizable model.</h5>\n",
    "<p>Cost Function with L2 Regularization:<br>\n",
    "The cost function J is modified to include a regularization term:\n",
    "\n",
    "$$J_{\\text{regularized}} = J_{\\text{original}} + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\|W^{[l]}\\|_F^2$$\n",
    "\n",
    "where J_originial is the original cost function(e.g. cross-entropy loss).<br>\n",
    "Œª is The regularization parameter that controls the strength of regularization.<br>\n",
    "m is Number of training examples.<br>\n",
    "and $$\\|W^{[l]}\\|_F^2$$ is The Frobenius norm (sum of squared elements) of the weight matrix W[l] for layer l.</p>\n",
    "<p>Large weights in a model can lead to overfitting, as the model may memorize the training data instead of learning general patterns. L2 regularization discourages large weights.<br>\n",
    "By penalizing large weights, L2 regularization encourages the model to rely on more features with small contributions rather than focusing heavily on a few.<br>\n",
    "A model with smaller weights is often better at making predictions on unseen data.</p>\n",
    "<p>Effect of Œª on Regularization:<br>\n",
    "Small Œª:<br>\n",
    "Weak regularization.<br>\n",
    "The model is more likely to overfit, as the weights can grow large.<br>\n",
    "Large Œª:<br>\n",
    "Strong regularization.<br>\n",
    "The weights are heavily penalized, which can lead to underfitting, as the model may fail to capture important patterns.<br>\n",
    "Tuning Œª:<br>\n",
    "Œª is typically tuned using a validation set or cross-validation to find the right balance between underfitting and overfitting.<br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Back propagation</h4>\n",
    "<h5>In this function wecomputes the gradients of the cost function with respect to the parameters (W and ùëè) of the network. These gradients are used during optimization (e.g gradient descent) to update the weights and biases.</h5>\n",
    "<p>In our function :<br>\n",
    "grads is A dictionary to store the gradients (dW and db) for each layer.<br>\n",
    "m is The number of training examples (used to compute averages).<br>\n",
    "and L is Total number of layers in the network.</p>\n",
    "<p>Final layer computations:<br>\n",
    "First, we retrieve Output Activations (AL):<br>\n",
    "AL is the output of the softmax function in the last layer, stored in cache[f\"A{L}\"]<br>\n",
    "we used L2 regularization as we described it above<br>\n",
    "During backpropagation, the gradients of the cost function with respect to the weights are modified to include the derivative of the regularization term.<br>\n",
    "Without Regularization:\n",
    "\n",
    "$$dW^{[l]} = \\frac{\\partial J}{\\partial W^{[l]}}$$\n",
    "With L2 Regularization:\n",
    "\n",
    "$$dW^{[l]} = \\frac{\\partial J_{\\text{original}}}{\\partial W^{[l]}} + \\frac{\\lambda}{m} W^{[l]}$$\n",
    "the last term adds a penalty proportional to the weights, encouraging smaller values.<br>\n",
    "Back to the operation in our function we Compute the Gradient of the Cost w.r.t. z[L]:\n",
    "$$dZ^{[L]} = A^{[L]} - Y$$\n",
    "This is derived from the derivative of the cross-entropy loss combined with the softmax activation function.<br>\n",
    "Then we compute Gradients for Weights and Biases:<br>\n",
    "For wieghts:<br>\n",
    "$$dW^{[L]} = \\frac{1}{m} \\cdot dZ^{[L]} \\cdot (A^{[L-1]})^\\top + \\frac{\\lambda}{m} \\cdot W^{[L]}$$\n",
    "Which The first term computes the gradient of the loss w.r.t. the weights and the second term adds L2 regularization to prevent overfitting.<br>\n",
    "for biases:<br>\n",
    "$$db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)}$$\n",
    "We discussed about Dw in above , for db:<br>\n",
    "We know For each training example i, the pre-activation Z [l] for a neuron is:<br>\n",
    "$$Z^{[l](i)} = W^{[l]} \\cdot A^{[l-1](i)} + b^{[l]}$$\n",
    "The cost function J is typically a sum over all training examples:<br>\n",
    "$$J = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Cost}^{(i)}$$\n",
    "The gradient of the cost J w.r.t. the bias b[l] is:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial J} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial b^{[l]}}{\\partial \\text{Cost}^{(i)}}$$\n",
    "For a single training example i, the cost depends on b[l] through z[l][i], so we use the chain rule:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial \\text{Cost}^{(i)}} = \\frac{\\partial Z^{[l](i)}}{\\partial \\text{Cost}^{(i)}} \\cdot \\frac{\\partial b^{[l]}}{\\partial Z^{[l](i)}}$$\n",
    "From the definition of z[l][i], the derivative w.r.t. b[l] is:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial Z^{[l](i)}} = 1$$\n",
    "hence:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial \\text{Cost}^{(i)}} = \\frac{\\partial Z^{[l](i)}}{\\partial \\text{Cost}^{(i)}} = dZ^{[l](i)}$$\n",
    "Since b [l] is a vector (with one bias term for each neuron in the layer), this formula generalizes to:\n",
    "$$db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)}$$\n",
    "<br>\n",
    "Then back to our Back propagation aperation we loop Backward Through Each Layer and we compute dA[l]<br>\n",
    "The gradient of the cost w.r.t. the activations of the current layer is computed using the chain rule:<br>\n",
    "\n",
    "$$dA^{[l]} = W^{[l+1]^\\top} \\cdot dZ^{[l+1]}$$\n",
    "\n",
    "We prove this in below:<br>\n",
    "Using the chain rule, dA [l] can be expressed as:<br>\n",
    "$$dA^{[l]} = \\frac{\\partial Z^{[l+1]}}{\\partial J} \\cdot \\frac{\\partial A^{[l]}}{\\partial Z^{[l+1]}}$$\n",
    "For computing second term we know The derivative of Z [l+1] with respect to A [l] is(based on the formula for z[l]):\n",
    "$$\\frac{\\partial A^{[l]}}{\\partial Z^{[l+1]}} = W^{[l+1]}$$\n",
    "So we have :\n",
    "$$dA^{[l]} = \\frac{\\partial Z^{[l+1]}}{\\partial J} \\cdot W^{[l+1]}$$\n",
    "Using the notation:\n",
    "$$dZ^{[l+1]} = \\frac{\\partial Z^{[l+1]}}{\\partial J}$$\n",
    "We rewrite:\n",
    "$$dA^{[l]} = W^{[l+1]^\\top} \\cdot dZ^{[l+1]}$$\n",
    "So this formula helps us propagates the error backward from the next layer.<br>\n",
    "We can also apply Dropout if needed by reversing the effect of dropout by multiplying with the dropout mask (D) and scaling back with dividing by keep_prob.<br>\n",
    "This ensures gradients flow only through the neurons that were active during the forward pass.<br>\n",
    "For computing dz[l] we have the formula:\n",
    "$$dZ^{[l]} = dA^{[l]} \\cdot \\text{ReLU}'(Z^{[l]})$$\n",
    "Let's prove it:<br>\n",
    "Using the chain rule:<br>\n",
    "$$dZ^{[l]} = \\frac{\\partial Z^{[l]}}{\\partial J} = \\frac{\\partial A^{[l]}}{\\partial J} \\cdot \\frac{\\partial Z^{[l]}}{\\partial A^{[l]}}$$\n",
    "We know the first term is equal to dA[l], the second term depends on the activation function which is ReLU(z[l]) in here so the final formulla for dz[l] is:\n",
    "$$dZ^{[l]} = dA^{[l]} \\cdot \\text{ReLU}'(Z^{[l]})$$\n",
    "Like the way we compute for the last layer we compute Gradients for Weights and Biases:<br>\n",
    "For weights:<br>\n",
    "$$dW^{[L]} = \\frac{1}{m} \\cdot dZ^{[L]} \\cdot (A^{[L-1]})^\\top + \\frac{\\lambda}{m} \\cdot W^{[L]}$$\n",
    "This is obviously true and be proved just like above by using chain rule so we won't cover it<br>\n",
    "And for db the formulla is just like the one for the last layer<br>\n",
    "At last, we store dW [l] and db [l] in the grads dictionary, and return the grads dictionary to use them to update weights and biases during optimization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache, parameters, keep_prob=1.0, lambda_=0.0):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    AL = cache[f\"A{L}\"]\n",
    "    dZL = cross_entropy_derivative(Y, AL)\n",
    "    grads[f\"dW{L}\"] = (1 / m) * np.dot(dZL, cache[f\"A{L-1}\"].T) + (lambda_ / m) * parameters[f\"W{L}\"]\n",
    "    grads[f\"db{L}\"] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        dA = np.dot(parameters[f\"W{l+1}\"].T, dZL)\n",
    "\n",
    "        if keep_prob < 1.0:\n",
    "            dA = dA * cache[f\"D{l}\"]\n",
    "            dA = dA / keep_prob\n",
    "\n",
    "        dZL = dA * relu_derivative(cache[f\"Z{l}\"])\n",
    "        grads[f\"dW{l}\"] = (1 / m) * np.dot(dZL, cache[f\"A{l-1}\"].T) + (lambda_ / m) * parameters[f\"W{l}\"]\n",
    "        grads[f\"db{l}\"] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Updating parameter</h4>\n",
    "<p> function is responsible for adjusting the weights and biases of a neural network using the gradients computed during backpropagation. This adjustment ensures that the model's predictions get closer to the true labels, thereby reducing the cost function J over time.<br>\n",
    "The function is an implementation of gradient descent<br>\n",
    "During backpropagation, the gradients \n",
    "\n",
    "$$\\frac{\\partial W^{[l]}}{\\partial J},\\frac{\\partial b^{[l]}}{\\partial J}$$\n",
    "are computed for each layer. These gradients tell us how much W[l] and b [l]need to change to reduce J.<br>\n",
    "Gradient descent updates the parameters by taking a step in the direction opposite to the gradient:<br>\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J$$\n",
    "where:<br>\n",
    "Œ∏ represents the parameters (W [l], b [l]).<br>\n",
    "‚àá_Œ∏J represents the gradient of J with respect to Œ∏.<br>\n",
    "Œ± is the learning rate, controlling the step size.<br>\n",
    "<br>\n",
    "The gradient ‚àá_Œ∏J indicates the direction of the steepest increase in J. Moving in the opposite direction reduces J.\n",
    "A small learning rate ensures stable convergence but slows down training.<br>\n",
    "A large learning rate speeds up training but risks overshooting the minimum.<br>\n",
    "Each update reduces J, and the process is repeated over many iterations (epochs) to converge to a minimum.<br>\n",
    "<br>\n",
    "In our function we update the weights by:<br>\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
    "where:<br>\n",
    "Œ±: Learning rate.<br>\n",
    "dW[l] : Indicates the direction and magnitude of change required for W[l] to minimize the cost function J.<br>\n",
    "And for biases:<br>\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
    "where:<br>\n",
    "Œ±: Learning rate.<br>\n",
    "db[l] : Indicates the direction and magnitude of change required for b[l] to minimize the cost function J.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>creating mini batches</h4>\n",
    "<h5>This function divides the training data into smaller subsets (mini-batches) for training. Mini-batches improve training efficiency and stability.</h5>\n",
    "<p>First, we shuffle the Data and randomize the order of the training examples by generating a random permutation of indices.<br>\n",
    "This will ensures that mini-batches contain different subsets of data in each epoch.<br>\n",
    "Then we split our Data into Mini-Batches,which means we Creates subsets of X and Y for each mini-batch<br>\n",
    "After all, We handle Leftover Examples:<br>\n",
    "If the total number of examples (m) is not divisible by the mini-batch size, it creates a smaller batch with the remaining examples.<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, Y, mini_batch_size):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    permutation = np.random.permutation(m)\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "    for k in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Training the model</h4>\n",
    "<h5>This is the main function that trains the neural network by iterating through the training data, performing forward propagation, backpropagation, and updating the weights and biases.</h5>\n",
    "<p>First, we call initialize_parameters(layer_dims) to initialize weights and biases for the network.<br>\n",
    "Then, we loop through a specified number of epochs (iterations over the full dataset).<br>\n",
    "For each epoch we:<br>\n",
    "Reduce the learning rate as the epochs progress.<br>\n",
    "Then, we call create_mini_batches to split the training data into smaller batches and shuffles them every time.<br>\n",
    "For each mini-batch we do:<br>\n",
    "Forward Propagation:to compute activations and stores intermediate values.<br>\n",
    "Backward Propagation:to compute gradients using the cost function.<br>\n",
    "Update Parameters:to adjust weights and biases using gradients.<br>\n",
    "After that, we compute loss (using cross_entropy_loss) and test accuracy at the end of each epoch and we print the loss and accuracy periodically.<br>\n",
    "In the end, we plot loss and accuracy trends over epochs.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(losses, accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, layer_dims, epochs, learning_rate, mini_batch_size, keep_prob, lambda_, decay_rate):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    m = X_train.shape[1]\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        learning_rate_epoch = learning_rate / (1 + decay_rate * epoch)\n",
    "        mini_batches = create_mini_batches(X_train, Y_train, mini_batch_size)\n",
    "\n",
    "        for mini_batch in mini_batches:\n",
    "            (mini_batch_X, mini_batch_Y) = mini_batch\n",
    "            \n",
    "            AL, cache = forward_propagation(mini_batch_X, parameters, keep_prob)\n",
    "            grads = backward_propagation(mini_batch_X, mini_batch_Y, cache, parameters, keep_prob, lambda_)\n",
    "            parameters = update_parameters(parameters, grads, learning_rate_epoch)\n",
    "\n",
    "        AL_train, _ = forward_propagation(X_train, parameters)\n",
    "        loss = cross_entropy_loss(Y_train, AL_train)\n",
    "        losses.append(loss)\n",
    "\n",
    "        AL_test, _ = forward_propagation(X_test, parameters)\n",
    "        predictions = np.argmax(AL_test, axis=0)\n",
    "        labels = np.argmax(Y_test, axis=0)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    plot_metrics(losses, accuracies)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>One-Hot Encoded Matrix<h4>\n",
    "<p>we need a function to make One-Hot Encoded Matrix from our data where each column corresponds to one training example, and each row corresponds to a class.<br>\n",
    "Neural networks output probabilities for each class (e.g., softmax). One-hot encoding aligns the target labels with this format.<br>\n",
    "Cross-entropy loss can directly compare the predicted probabilities to the one-hot encoded labels.<br>\n",
    "First we initialize a One-Hot Matrix:<br>\n",
    "np.zeros((num_classes, m)) creates a matrix of zeros with:<br>\n",
    "num_classes: Number of possible output classes (for our network 10 for digits 0‚Äì9).<br>\n",
    "m: Number of examples in the dataset.<br>\n",
    "Ten we encode Labels:<br>\n",
    "labels: An array of shape (m,) containing the actual class labels for each example.<br>\n",
    "one_hot[labels, np.arange(m)] = 1:<br>\n",
    "For each example i, it sets the value at one_hot[labels[i], i] to 1.<br>\n",
    "This effectively \"activates\" the correct class for each example in the one-hot matrix.<br>\n",
    "After that, We return the One-Hot Encoded Matrix:<br>\n",
    "The resulting matrix has shape (num_classes, m), where each column corresponds to one training example, and each row corresponds to a class.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    m = labels.shape[0]\n",
    "    one_hot = np.zeros((num_classes, m))\n",
    "    one_hot[labels, np.arange(m)] = 1\n",
    "    return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
