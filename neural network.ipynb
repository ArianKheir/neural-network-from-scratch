{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a Neural network from scratch</h1>\n",
    "<p>We are going to use just numpy for calculations with matrices and matplotlib.pyplot for showing results on charts</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU:</h4>\n",
    "<p>We use ReLU to set all the negative inputs to zero and leave positive inputs unchanged.<br>\n",
    "It's simplicity makes it computationally efficient, especially in large neural networks.<br>\n",
    "ReLU introduces non-linearity into the network, allowing it to learn complex patterns.<br>\n",
    "For z > 0 the gradient is constant (dReLu(z)/dz =1), which help us avoid the vanishing gradient problem (which is common in sigmoid/tanh). <br>\n",
    "Also it sets some activations to zero, which can help with computational efficiency and reduce the risk of overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Softmax Function:</h4>\n",
    "<p>It converts raw scores z(logits) into probabilities and also ensures that the probabilities sum to 1 across all classes.<br>\n",
    "Exponentiating the logits ensures all values are positive and also Larger logits get amplified more than smaller logits.<br>\n",
    "Dividing by the sum of all exponentiated logits ensures the output values are probabilities is also some kind of normalization.The subtraction of max(Z) prevents numerical overflow(e.g for large exponentials).</p>\n",
    "<p>In code below axis=0 ensures softmax is computed for each column(e.g for each class in a multi-class classification setting).<br>\n",
    "And Also keepdims=True ensures that the result sum has the same dimensions as expZ, allowing for element-wise division without broadcasting issues.</p>\n",
    "<h5>fromula for it is:</h5>\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross entropy loss function</h4>\n",
    "<p>This function computes the cross-entropy loss, which measures the difference between the predicted probabilities (y_pred) and the true labels (y_true). It is commonly used in classification tasks, especially for multi-class classification with softmax outputs.</p>\n",
    "<h5>fromula for this loss function is:</h5>\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$\n",
    "\n",
    "If yk = 1(the correct class), the loss for that sample is equal to:\n",
    "\n",
    "$$L = -\\log(\\hat{y}_k)$$\n",
    "Encourages hat{yk} (the predicted probability for the correct class) to approach 1.<br>\n",
    "The small value 1e-8 is added to hat{yk} to prevent taking the logarithm of 0, which would result in an undefined value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>When Combined with Softmax Output:</h5>\n",
    "<p>After simplifying, the gradient of the combined softmax and cross-entropy loss is:<br></p>\n",
    "\n",
    "$$\\frac{\\partial z_k^{(i)}}{\\partial J} = \\hat{y}_k^{(i)} - y_k^{(i)}$$\n",
    "<p>And when we Stack the derivatives for all m examples into a matrix: </p>\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial J} = \\hat{Y} - Y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[1]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Initializing the weights and biases</h4>\n",
    "<p>Proper initialization of weights is critical for training the network effectively, as it impacts the convergence speed and overall performance.<br>\n",
    "Poor initialization (e.g., very large or very small weights) can cause gradients to shrink (vanish) or grow (explode) exponentially as they propagate through layers.<br>\n",
    "We set a seed for NumPy's random number generator to ensure reproducibility.<br>\n",
    "Using the same seed produces the same random numbers every time the code runs, which is useful for debugging or comparing results.<br>\n",
    "For initializing weights we use He initialization.<br>\n",
    "For each weight we generate random values from a standard normal distribution(N(0, 1)) and scale it by * np.sqrt(2 / layer_dims[l-1])<br>\n",
    "Using He initialization ensures the variance of the activations is maintained as the signal passes through layers, avoiding the problems of vanishing or exploding gradients<br></p>\n",
    "<p>We initialize the biases to zeros by np.zeros()<br>\n",
    "This doesn’t break symmetry, unlike initializing weights to zeros (which would cause all neurons in the layer to learn identical features).</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dropout</h4>\n",
    "<p>Dropout is a regularization technique where a random subset of neurons is \"dropped\" (set to 0) during training. This prevents the network from becoming too reliant on specific neurons and helps generalization.<br>\n",
    "the process works like below:<br>\n",
    "if dropout is enabled (keep_prob < 1.0):<br>\n",
    "We create a random matrix D where each element is True with probability keep_prob and False otherwise.<br>\n",
    "Then we drop some neurons in A by element-wise multiplying A and D and after that we scale the remaining activations by dividing by keep_prob.This will help us to maintain the expected value of the activations.After all, we store the dropout mask D in the cache.<br>\n",
    "<h5>There are many benefits using this regularization technique:</h5>\n",
    "<p>By randomly deactivating neurons, dropout prevents the network from relying too much on specific neurons.<br>\n",
    "Also this technique forces the network to learn redundant representations since no single neuron can dominate.<br></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward propagation</h4>\n",
    "<h5>In this function we calculates the outputs of each layer in the neural network from the input layer to the output layer.</h5>\n",
    "<p>L is the total number of layers in the network (excluding the input layer). Each layer has weights and biases, hence parameters contains 2 * L keys.<br>\n",
    "A is initially set to the input matrix X and cache is used to store intermediate values (activations A and pre-activations Z) for each layer, which will be needed during backward propagation.<br>\n",
    "For each layer l we compute the pre-activation Z using the formula:<br>\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "where W[l] is the weight matrix for layer l.<br> A[l−1] is the activation from the previous layer (or input X for the first layer) <br>and b[l] is the bias vector for layer l.<br>\n",
    "Then we compute the activation A using the ReLU activation function:\n",
    "$$A^{[l]} = \\text{ReLU}(Z^{[l]})$$\n",
    "We also use Dropout Regularization the way we described above<br></p>\n",
    "<p>in final layer:</p>\n",
    "<p>we compute the pre-activation ZL for the final layer using the same formula as above.\n",
    "then we compute the activation AL using the softmax function<br>\n",
    "After all, We store the pre-activation ZL and activation AL for the final layer in the cache and Al will be our final output of the network</p>\n",
    "<p>To put it in a nutshell:<br>\n",
    "    We applie the weight and bias transformations(Z) layer-by-layer, starting from the input layer and moving toward the output layer.<br>\n",
    "    Then we use non-linear activation functions (ReLU, softmax) to introduce non-linearity, allowing the network to model complex relationships.<br>\n",
    "    Also we optionally use dropout to prevent overfitting by randomly deactivating neurons during training.<br>\n",
    "    And in the end we store all intermediate values in cache, ensuring the data needed for backpropagation is available<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob=1.0):\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "    cache = {\"A0\": A}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters[f\"W{l}\"], A) + parameters[f\"b{l}\"]\n",
    "        A = relu(Z)\n",
    "        \n",
    "        if keep_prob < 1.0:\n",
    "            D = np.random.rand(A.shape[0], A.shape[1]) < keep_prob\n",
    "            A = A * D\n",
    "            A = A / keep_prob\n",
    "            cache[f\"D{l}\"] = D\n",
    "        \n",
    "        cache[f\"Z{l}\"] = Z\n",
    "        cache[f\"A{l}\"] = A\n",
    "\n",
    "    ZL = np.dot(parameters[f\"W{L}\"], A) + parameters[f\"b{L}\"]\n",
    "    AL = softmax(ZL)\n",
    "    \n",
    "    cache[f\"Z{L}\"] = ZL\n",
    "    cache[f\"A{L}\"] = AL\n",
    "    return AL, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>L2 regularization</h4>\n",
    "<h5>L2 regularization, also known as weight decay, is a technique used in machine learning to prevent overfitting by discouraging large weights in the model. It works by adding a penalty term to the cost function, which is proportional to the sum of the squared weights of the model. This encourages the model to keep the weights small, leading to a simpler and more generalizable model.</h5>\n",
    "<p>Cost Function with L2 Regularization:<br>\n",
    "The cost function J is modified to include a regularization term:\n",
    "\n",
    "$$J_{\\text{regularized}} = J_{\\text{original}} + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\|W^{[l]}\\|_F^2$$\n",
    "\n",
    "where J_originial is the original cost function(e.g. cross-entropy loss).<br>\n",
    "λ is The regularization parameter that controls the strength of regularization.<br>\n",
    "m is Number of training examples.<br>\n",
    "and $$\\|W^{[l]}\\|_F^2$$ is The Frobenius norm (sum of squared elements) of the weight matrix W[l] for layer l.</p>\n",
    "<p>Large weights in a model can lead to overfitting, as the model may memorize the training data instead of learning general patterns. L2 regularization discourages large weights.<br>\n",
    "By penalizing large weights, L2 regularization encourages the model to rely on more features with small contributions rather than focusing heavily on a few.<br>\n",
    "A model with smaller weights is often better at making predictions on unseen data.</p>\n",
    "<p>Effect of λ on Regularization:<br>\n",
    "Small λ:<br>\n",
    "Weak regularization.<br>\n",
    "The model is more likely to overfit, as the weights can grow large.<br>\n",
    "Large λ:<br>\n",
    "Strong regularization.<br>\n",
    "The weights are heavily penalized, which can lead to underfitting, as the model may fail to capture important patterns.<br>\n",
    "Tuning λ:<br>\n",
    "λ is typically tuned using a validation set or cross-validation to find the right balance between underfitting and overfitting.<br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Back propagation</h4>\n",
    "<h5>In this function wecomputes the gradients of the cost function with respect to the parameters (W and 𝑏) of the network. These gradients are used during optimization (e.g gradient descent) to update the weights and biases.</h5>\n",
    "<p>In our function :<br>\n",
    "grads is A dictionary to store the gradients (dW and db) for each layer.<br>\n",
    "m is The number of training examples (used to compute averages).<br>\n",
    "and L is Total number of layers in the network.</p>\n",
    "<p>Final layer computations:<br>\n",
    "First, we retrieve Output Activations (AL):<br>\n",
    "AL is the output of the softmax function in the last layer, stored in cache[f\"A{L}\"]<br>\n",
    "we used L2 regularization as we described it above<br>\n",
    "During backpropagation, the gradients of the cost function with respect to the weights are modified to include the derivative of the regularization term.<br>\n",
    "Without Regularization:\n",
    "\n",
    "$$dW^{[l]} = \\frac{\\partial J}{\\partial W^{[l]}}$$\n",
    "With L2 Regularization:\n",
    "\n",
    "$$dW^{[l]} = \\frac{\\partial J_{\\text{original}}}{\\partial W^{[l]}} + \\frac{\\lambda}{m} W^{[l]}$$\n",
    "the last term adds a penalty proportional to the weights, encouraging smaller values.<br>\n",
    "Back to the operation in our function we Compute the Gradient of the Cost w.r.t. z[L]:\n",
    "$$dZ^{[L]} = A^{[L]} - Y$$\n",
    "This is derived from the derivative of the cross-entropy loss combined with the softmax activation function.<br>\n",
    "Then we compute Gradients for Weights and Biases:<br>\n",
    "For wieghts:<br>\n",
    "$$dW^{[L]} = \\frac{1}{m} \\cdot dZ^{[L]} \\cdot (A^{[L-1]})^\\top + \\frac{\\lambda}{m} \\cdot W^{[L]}$$\n",
    "Which The first term computes the gradient of the loss w.r.t. the weights and the second term adds L2 regularization to prevent overfitting.<br>\n",
    "for biases:<br>\n",
    "$$db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)}$$\n",
    "We discussed about Dw in above , for db:<br>\n",
    "We know For each training example i, the pre-activation Z [l] for a neuron is:<br>\n",
    "$$Z^{[l](i)} = W^{[l]} \\cdot A^{[l-1](i)} + b^{[l]}$$\n",
    "The cost function J is typically a sum over all training examples:<br>\n",
    "$$J = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Cost}^{(i)}$$\n",
    "The gradient of the cost J w.r.t. the bias b[l] is:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial J} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial b^{[l]}}{\\partial \\text{Cost}^{(i)}}$$\n",
    "For a single training example i, the cost depends on b[l] through z[l][i], so we use the chain rule:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial \\text{Cost}^{(i)}} = \\frac{\\partial Z^{[l](i)}}{\\partial \\text{Cost}^{(i)}} \\cdot \\frac{\\partial b^{[l]}}{\\partial Z^{[l](i)}}$$\n",
    "From the definition of z[l][i], the derivative w.r.t. b[l] is:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial Z^{[l](i)}} = 1$$\n",
    "hence:\n",
    "$$\\frac{\\partial b^{[l]}}{\\partial \\text{Cost}^{(i)}} = \\frac{\\partial Z^{[l](i)}}{\\partial \\text{Cost}^{(i)}} = dZ^{[l](i)}$$\n",
    "Since b [l] is a vector (with one bias term for each neuron in the layer), this formula generalizes to:\n",
    "$$db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)}$$\n",
    "<br>\n",
    "Then back to our Back propagation aperation we loop Backward Through Each Layer and we compute dA[l]<br>\n",
    "The gradient of the cost w.r.t. the activations of the current layer is computed using the chain rule:<br>\n",
    "\n",
    "$$dA^{[l]} = W^{[l+1]^\\top} \\cdot dZ^{[l+1]}$$\n",
    "\n",
    "We prove this in below:<br>\n",
    "Using the chain rule, dA [l] can be expressed as:<br>\n",
    "$$dA^{[l]} = \\frac{\\partial Z^{[l+1]}}{\\partial J} \\cdot \\frac{\\partial A^{[l]}}{\\partial Z^{[l+1]}}$$\n",
    "For computing second term we know The derivative of Z [l+1] with respect to A [l] is(based on the formula for z[l]):\n",
    "$$\\frac{\\partial A^{[l]}}{\\partial Z^{[l+1]}} = W^{[l+1]}$$\n",
    "So we have :\n",
    "$$dA^{[l]} = \\frac{\\partial Z^{[l+1]}}{\\partial J} \\cdot W^{[l+1]}$$\n",
    "Using the notation:\n",
    "$$dZ^{[l+1]} = \\frac{\\partial Z^{[l+1]}}{\\partial J}$$\n",
    "We rewrite:\n",
    "$$dA^{[l]} = W^{[l+1]^\\top} \\cdot dZ^{[l+1]}$$\n",
    "So this formula helps us propagates the error backward from the next layer.<br>\n",
    "We can also apply Dropout if needed by reversing the effect of dropout by multiplying with the dropout mask (D) and scaling back with dividing by keep_prob.<br>\n",
    "This ensures gradients flow only through the neurons that were active during the forward pass.<br>\n",
    "For computing dz[l] we have the formula:\n",
    "$$dZ^{[l]} = dA^{[l]} \\cdot \\text{ReLU}'(Z^{[l]})$$\n",
    "Let's prove it:<br>\n",
    "Using the chain rule:<br>\n",
    "$$dZ^{[l]} = \\frac{\\partial Z^{[l]}}{\\partial J} = \\frac{\\partial A^{[l]}}{\\partial J} \\cdot \\frac{\\partial Z^{[l]}}{\\partial A^{[l]}}$$\n",
    "We know the first term is equal to dA[l], the second term depends on the activation function which is ReLU(z[l]) in here so the final formulla for dz[l] is:\n",
    "$$dZ^{[l]} = dA^{[l]} \\cdot \\text{ReLU}'(Z^{[l]})$$\n",
    "Like the way we compute for the last layer we compute Gradients for Weights and Biases:<br>\n",
    "For weights:<br>\n",
    "$$dW^{[L]} = \\frac{1}{m} \\cdot dZ^{[L]} \\cdot (A^{[L-1]})^\\top + \\frac{\\lambda}{m} \\cdot W^{[L]}$$\n",
    "This is obviously true and be proved just like above by using chain rule so we won't cover it<br>\n",
    "And for db the formulla is just like the one for the last layer<br>\n",
    "At last, we store dW [l] and db [l] in the grads dictionary, and return the grads dictionary to use them to update weights and biases during optimization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache, parameters, keep_prob=1.0, lambda_=0.0):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    AL = cache[f\"A{L}\"]\n",
    "    dZL = cross_entropy_derivative(Y, AL)\n",
    "    grads[f\"dW{L}\"] = (1 / m) * np.dot(dZL, cache[f\"A{L-1}\"].T) + (lambda_ / m) * parameters[f\"W{L}\"]\n",
    "    grads[f\"db{L}\"] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        dA = np.dot(parameters[f\"W{l+1}\"].T, dZL)\n",
    "\n",
    "        if keep_prob < 1.0:\n",
    "            dA = dA * cache[f\"D{l}\"]\n",
    "            dA = dA / keep_prob\n",
    "\n",
    "        dZL = dA * relu_derivative(cache[f\"Z{l}\"])\n",
    "        grads[f\"dW{l}\"] = (1 / m) * np.dot(dZL, cache[f\"A{l-1}\"].T) + (lambda_ / m) * parameters[f\"W{l}\"]\n",
    "        grads[f\"db{l}\"] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Updating parameter</h4>\n",
    "<p> function is responsible for adjusting the weights and biases of a neural network using the gradients computed during backpropagation. This adjustment ensures that the model's predictions get closer to the true labels, thereby reducing the cost function J over time.<br>\n",
    "The function is an implementation of gradient descent<br>\n",
    "During backpropagation, the gradients \n",
    "\n",
    "$$\\frac{\\partial W^{[l]}}{\\partial J},\\frac{\\partial b^{[l]}}{\\partial J}$$\n",
    "are computed for each layer. These gradients tell us how much W[l] and b [l]need to change to reduce J.<br>\n",
    "Gradient descent updates the parameters by taking a step in the direction opposite to the gradient:<br>\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J$$\n",
    "where:<br>\n",
    "θ represents the parameters (W [l], b [l]).<br>\n",
    "∇_θJ represents the gradient of J with respect to θ.<br>\n",
    "α is the learning rate, controlling the step size.<br>\n",
    "<br>\n",
    "The gradient ∇_θJ indicates the direction of the steepest increase in J. Moving in the opposite direction reduces J.\n",
    "A small learning rate ensures stable convergence but slows down training.<br>\n",
    "A large learning rate speeds up training but risks overshooting the minimum.<br>\n",
    "Each update reduces J, and the process is repeated over many iterations (epochs) to converge to a minimum.<br>\n",
    "<br>\n",
    "In our function we update the weights by:<br>\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
    "where:<br>\n",
    "α: Learning rate.<br>\n",
    "dW[l] : Indicates the direction and magnitude of change required for W[l] to minimize the cost function J.<br>\n",
    "And for biases:<br>\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
    "where:<br>\n",
    "α: Learning rate.<br>\n",
    "db[l] : Indicates the direction and magnitude of change required for b[l] to minimize the cost function J.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>creating mini batches</h4>\n",
    "<h5>This function divides the training data into smaller subsets (mini-batches) for training. Mini-batches improve training efficiency and stability.</h5>\n",
    "<p>First, we shuffle the Data and randomize the order of the training examples by generating a random permutation of indices.<br>\n",
    "This will ensures that mini-batches contain different subsets of data in each epoch.<br>\n",
    "Then we split our Data into Mini-Batches,which means we Creates subsets of X and Y for each mini-batch<br>\n",
    "After all, We handle Leftover Examples:<br>\n",
    "If the total number of examples (m) is not divisible by the mini-batch size, it creates a smaller batch with the remaining examples.<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, Y, mini_batch_size):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    permutation = np.random.permutation(m)\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "    for k in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Training the model</h4>\n",
    "<h5>This is the main function that trains the neural network by iterating through the training data, performing forward propagation, backpropagation, and updating the weights and biases.</h5>\n",
    "<p>First, we call initialize_parameters(layer_dims) to initialize weights and biases for the network.<br>\n",
    "Then, we loop through a specified number of epochs (iterations over the full dataset).<br>\n",
    "For each epoch we:<br>\n",
    "Reduce the learning rate as the epochs progress.<br>\n",
    "Then, we call create_mini_batches to split the training data into smaller batches and shuffles them every time.<br>\n",
    "For each mini-batch we do:<br>\n",
    "Forward Propagation:to compute activations and stores intermediate values.<br>\n",
    "Backward Propagation:to compute gradients using the cost function.<br>\n",
    "Update Parameters:to adjust weights and biases using gradients.<br>\n",
    "After that, we compute loss (using cross_entropy_loss) and test accuracy at the end of each epoch and we print the loss and accuracy periodically.<br>\n",
    "In the end, we plot loss and accuracy trends over epochs.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(losses, accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, layer_dims, epochs, learning_rate, mini_batch_size, keep_prob, lambda_, decay_rate):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    m = X_train.shape[1]\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        learning_rate_epoch = learning_rate / (1 + decay_rate * epoch)\n",
    "        mini_batches = create_mini_batches(X_train, Y_train, mini_batch_size)\n",
    "\n",
    "        for mini_batch in mini_batches:\n",
    "            (mini_batch_X, mini_batch_Y) = mini_batch\n",
    "            \n",
    "            AL, cache = forward_propagation(mini_batch_X, parameters, keep_prob)\n",
    "            grads = backward_propagation(mini_batch_X, mini_batch_Y, cache, parameters, keep_prob, lambda_)\n",
    "            parameters = update_parameters(parameters, grads, learning_rate_epoch)\n",
    "\n",
    "        AL_train, _ = forward_propagation(X_train, parameters)\n",
    "        loss = cross_entropy_loss(Y_train, AL_train)\n",
    "        losses.append(loss)\n",
    "\n",
    "        AL_test, _ = forward_propagation(X_test, parameters)\n",
    "        predictions = np.argmax(AL_test, axis=0)\n",
    "        labels = np.argmax(Y_test, axis=0)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    plot_metrics(losses, accuracies)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>One-Hot Encoded Matrix<h4>\n",
    "<p>we need a function to make One-Hot Encoded Matrix from our data where each column corresponds to one training example, and each row corresponds to a class.<br>\n",
    "Neural networks output probabilities for each class (e.g., softmax). One-hot encoding aligns the target labels with this format.<br>\n",
    "Cross-entropy loss can directly compare the predicted probabilities to the one-hot encoded labels.<br>\n",
    "First we initialize a One-Hot Matrix:<br>\n",
    "np.zeros((num_classes, m)) creates a matrix of zeros with:<br>\n",
    "num_classes: Number of possible output classes (for our network 10 for digits 0–9).<br>\n",
    "m: Number of examples in the dataset.<br>\n",
    "Ten we encode Labels:<br>\n",
    "labels: An array of shape (m,) containing the actual class labels for each example.<br>\n",
    "one_hot[labels, np.arange(m)] = 1:<br>\n",
    "For each example i, it sets the value at one_hot[labels[i], i] to 1.<br>\n",
    "This effectively \"activates\" the correct class for each example in the one-hot matrix.<br>\n",
    "After that, We return the One-Hot Encoded Matrix:<br>\n",
    "The resulting matrix has shape (num_classes, m), where each column corresponds to one training example, and each row corresponds to a class.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    m = labels.shape[0]\n",
    "    one_hot = np.zeros((num_classes, m))\n",
    "    one_hot[labels, np.arange(m)] = 1\n",
    "    return one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
