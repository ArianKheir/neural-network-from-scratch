{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a Neural network from scratch</h1>\n",
    "<p>We are going to use just numpy for calculations with matrices and matplotlib.pyplot for showing results on charts</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU:</h4>\n",
    "<p>We use ReLU to set all the negative inputs to zero and leave positive inputs unchanged.<br>\n",
    "It's simplicity makes it computationally efficient, especially in large neural networks.<br>\n",
    "ReLU introduces non-linearity into the network, allowing it to learn complex patterns.<br>\n",
    "For z > 0 the gradient is constant (dReLu(z)/dz =1), which help us avoid the vanishing gradient problem (which is common in sigmoid/tanh). <br>\n",
    "Also it sets some activations to zero, which can help with computational efficiency and reduce the risk of overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Softmax Function:</h4>\n",
    "<p>It converts raw scores z(logits) into probabilities and also ensures that the probabilities sum to 1 across all classes.<br>\n",
    "Exponentiating the logits ensures all values are positive and also Larger logits get amplified more than smaller logits.<br>\n",
    "Dividing by the sum of all exponentiated logits ensures the output values are probabilities is also some kind of normalization.The subtraction of max(Z) prevents numerical overflow(e.g for large exponentials).</p>\n",
    "<p>In code below axis=0 ensures softmax is computed for each column(e.g for each class in a multi-class classification setting).<br>\n",
    "And Also keepdims=True ensures that the result sum has the same dimensions as expZ, allowing for element-wise division without broadcasting issues.</p>\n",
    "<h5>fromula for it is:</h5>\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
