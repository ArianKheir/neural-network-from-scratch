{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a Neural network from scratch</h1>\n",
    "<p>We are going to use just numpy for calculations with matrices and matplotlib.pyplot for showing results on charts</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU:</h4>\n",
    "<p>We use ReLU to set all the negative inputs to zero and leave positive inputs unchanged.<br>\n",
    "It's simplicity makes it computationally efficient, especially in large neural networks.<br>\n",
    "ReLU introduces non-linearity into the network, allowing it to learn complex patterns.<br>\n",
    "For z > 0 the gradient is constant (dReLu(z)/dz =1), which help us avoid the vanishing gradient problem (which is common in sigmoid/tanh). <br>\n",
    "Also it sets some activations to zero, which can help with computational efficiency and reduce the risk of overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Softmax Function:</h4>\n",
    "<p>It converts raw scores z(logits) into probabilities and also ensures that the probabilities sum to 1 across all classes.<br>\n",
    "Exponentiating the logits ensures all values are positive and also Larger logits get amplified more than smaller logits.<br>\n",
    "Dividing by the sum of all exponentiated logits ensures the output values are probabilities is also some kind of normalization.The subtraction of max(Z) prevents numerical overflow(e.g for large exponentials).</p>\n",
    "<p>In code below axis=0 ensures softmax is computed for each column(e.g for each class in a multi-class classification setting).<br>\n",
    "And Also keepdims=True ensures that the result sum has the same dimensions as expZ, allowing for element-wise division without broadcasting issues.</p>\n",
    "<h5>fromula for it is:</h5>\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross entropy loss function</h4>\n",
    "<p>This function computes the cross-entropy loss, which measures the difference between the predicted probabilities (y_pred) and the true labels (y_true). It is commonly used in classification tasks, especially for multi-class classification with softmax outputs.</p>\n",
    "<h5>fromula for this loss function is:</h5>\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$\n",
    "\n",
    "If yk = 1(the correct class), the loss for that sample is equal to:\n",
    "\n",
    "$$L = -\\log(\\hat{y}_k)$$\n",
    "Encourages hat{yk} (the predicted probability for the correct class) to approach 1.<br>\n",
    "The small value 1e-8 is added to hat{yk} to prevent taking the logarithm of 0, which would result in an undefined value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>When Combined with Softmax Output:</h5>\n",
    "<p>After simplifying, the gradient of the combined softmax and cross-entropy loss is:<br></p>\n",
    "\n",
    "$$\\frac{\\partial z_k^{(i)}}{\\partial J} = \\hat{y}_k^{(i)} - y_k^{(i)}$$\n",
    "<p>And when we Stack the derivatives for all m examples into a matrix: </p>\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial J} = \\hat{Y} - Y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[1]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Initializing the weights and biases</h4>\n",
    "<p>Proper initialization of weights is critical for training the network effectively, as it impacts the convergence speed and overall performance.<br>\n",
    "Poor initialization (e.g., very large or very small weights) can cause gradients to shrink (vanish) or grow (explode) exponentially as they propagate through layers.<br>\n",
    "We set a seed for NumPy's random number generator to ensure reproducibility.<br>\n",
    "Using the same seed produces the same random numbers every time the code runs, which is useful for debugging or comparing results.<br>\n",
    "For initializing weights we use He initialization.<br>\n",
    "For each weight we generate random values from a standard normal distribution(N(0, 1)) and scale it by * np.sqrt(2 / layer_dims[l-1])<br>\n",
    "Using He initialization ensures the variance of the activations is maintained as the signal passes through layers, avoiding the problems of vanishing or exploding gradients<br></p>\n",
    "<p>We initialize the biases to zeros by np.zeros()<br>\n",
    "This doesn’t break symmetry, unlike initializing weights to zeros (which would cause all neurons in the layer to learn identical features).</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dropout</h4>\n",
    "<p>Dropout is a regularization technique where a random subset of neurons is \"dropped\" (set to 0) during training. This prevents the network from becoming too reliant on specific neurons and helps generalization.<br>\n",
    "the process works like below:<br>\n",
    "if dropout is enabled (keep_prob < 1.0):<br>\n",
    "We create a random matrix D where each element is True with probability keep_prob and False otherwise.<br>\n",
    "Then we drop some neurons in A by element-wise multiplying A and D and after that we scale the remaining activations by dividing by keep_prob.This will help us to maintain the expected value of the activations.After all, we store the dropout mask D in the cache.<br>\n",
    "<h5>There are many benefits using this regularization technique:</h5>\n",
    "<p>By randomly deactivating neurons, dropout prevents the network from relying too much on specific neurons.<br>\n",
    "Also this technique forces the network to learn redundant representations since no single neuron can dominate.<br></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward propagation</h4>\n",
    "<h5>In this function we calculates the outputs of each layer in the neural network from the input layer to the output layer.</h5>\n",
    "<p>L is the total number of layers in the network (excluding the input layer). Each layer has weights and biases, hence parameters contains 2 * L keys.<br>\n",
    "A is initially set to the input matrix X and cache is used to store intermediate values (activations A and pre-activations Z) for each layer, which will be needed during backward propagation.<br>\n",
    "For each layer l we compute the pre-activation Z using the formula:<br>\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "where W[l] is the weight matrix for layer l.<br> A[l−1] is the activation from the previous layer (or input X for the first layer) <br>and b[l] is the bias vector for layer l.<br>\n",
    "Then we compute the activation A using the ReLU activation function:\n",
    "$$A^{[l]} = \\text{ReLU}(Z^{[l]})$$\n",
    "We also use Dropout Regularization the way we described above<br></p>\n",
    "<p>in final layer:</p>\n",
    "<p>we compute the pre-activation ZL for the final layer using the same formula as above.\n",
    "then we compute the activation AL using the softmax function<br>\n",
    "After all, We store the pre-activation ZL and activation AL for the final layer in the cache and Al will be our final output of the network</p>\n",
    "<p>To put it in a nutshell:<br>\n",
    "    We applie the weight and bias transformations(Z) layer-by-layer, starting from the input layer and moving toward the output layer.<br>\n",
    "    Then we use non-linear activation functions (ReLU, softmax) to introduce non-linearity, allowing the network to model complex relationships.<br>\n",
    "    Also we optionally use dropout to prevent overfitting by randomly deactivating neurons during training.<br>\n",
    "    And in the end we store all intermediate values in cache, ensuring the data needed for backpropagation is available<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob=1.0):\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "    cache = {\"A0\": A}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters[f\"W{l}\"], A) + parameters[f\"b{l}\"]\n",
    "        A = relu(Z)\n",
    "        \n",
    "        if keep_prob < 1.0:\n",
    "            D = np.random.rand(A.shape[0], A.shape[1]) < keep_prob\n",
    "            A = A * D\n",
    "            A = A / keep_prob\n",
    "            cache[f\"D{l}\"] = D\n",
    "        \n",
    "        cache[f\"Z{l}\"] = Z\n",
    "        cache[f\"A{l}\"] = A\n",
    "\n",
    "    ZL = np.dot(parameters[f\"W{L}\"], A) + parameters[f\"b{L}\"]\n",
    "    AL = softmax(ZL)\n",
    "    \n",
    "    cache[f\"Z{L}\"] = ZL\n",
    "    cache[f\"A{L}\"] = AL\n",
    "    return AL, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>L2 regularization</h4>\n",
    "<h5>L2 regularization, also known as weight decay, is a technique used in machine learning to prevent overfitting by discouraging large weights in the model. It works by adding a penalty term to the cost function, which is proportional to the sum of the squared weights of the model. This encourages the model to keep the weights small, leading to a simpler and more generalizable model.</h5>\n",
    "<p>Cost Function with L2 Regularization:<br>\n",
    "The cost function J is modified to include a regularization term:\n",
    "\n",
    "$$J_{\\text{regularized}} = J_{\\text{original}} + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\|W^{[l]}\\|_F^2$$\n",
    "\n",
    "where J_originial is the original cost function(e.g. cross-entropy loss).<br>\n",
    "λ is The regularization parameter that controls the strength of regularization.<br>\n",
    "m is Number of training examples.<br>\n",
    "and $$\\|W^{[l]}\\|_F^2$$ is The Frobenius norm (sum of squared elements) of the weight matrix W[l] for layer l.</p>\n",
    "<p>Large weights in a model can lead to overfitting, as the model may memorize the training data instead of learning general patterns. L2 regularization discourages large weights.<br>\n",
    "By penalizing large weights, L2 regularization encourages the model to rely on more features with small contributions rather than focusing heavily on a few.<br>\n",
    "A model with smaller weights is often better at making predictions on unseen data.</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
